<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Building SpInt</title><link>/</link><description></description><atom:link href="/feed.xml" rel="self"></atom:link><lastBuildDate>Fri, 03 Jun 2016 16:12:00 -0700</lastBuildDate><item><title>Generalized Linear Models and Sparse Design Matrices</title><link>/sparse_glm.html</link><description>&lt;p&gt;This week was primarily focused on exploring different options for estimating generalized linear models (GLM). First, I built an option into the gravity model class that would use exisitng GLM code through the statmodels project. Next, I put together my own code to carry out iteratively weighted least squares estimation for GLM's. Of immediate interest was a comparison of the speed of the code-bases for carrying out the estimation of a GLM. Each technique was used to calibrate each model variety (unconstrained, production-constrainedm atttraction-constrainem, and doubly constrained) when the number of origin-destination pairs (sample size) was N = 50, 100, 150, and 200. Some quick results can be see here in this &lt;a class="reference external" href="https://gist.github.com/TaylorOshan/ccac7e5489b5b2b1799bf79ef001d922"&gt;gist&lt;/a&gt;. Expectedly, the results show that for either technique the unconstrained models are the fastest to calibrate (no fixed effects), followed the by the singly-constrained models (N fixed effects), and finally, the doubly-constrained models take the longest (N*2 fixed effects). The more fixed effects in the model, the larger the deisgn matrix, X, and the longer the estimation routines will take. More surprisingly, the custom GLM (from now on just GLM) was way faster than the statsmodels GLM (from now on SMGLM). The statsmodels uses either the pseudoinverse or a QR decomposiiton to compute the least sqaures estimator, which would have been thought to be way quicker than the direct computations used in my code. For now, I have only tested the pseudo-indverse SMGLM, as the flag to switch to QR decomposition is not actually available to the SMGLM api (on to the weight least sqaures class at a lower level of abstraction). Perhaps, the SMGLM takes longer to compute because it has a fuller suite of diagnostics or perhaps the psuedo-inverse is not as quick when there is a sparse design matrix (i.e., many fixed effects).&lt;/p&gt;
&lt;p&gt;After some additional exploring, I found that there was an upper limit to the
number of locations (N = locations**2) that could considered for the
singly-constrained or doubly-constrained models given the high number of fixed
effects. Somewhere between 1000 and 2000 locations, my notebook would run out of
memory. To cirvument this, I next developed a version of the custom GLM code
that was compatible with the sparse data strcutres from the Scipy library, which
cna be found in this &lt;a class="reference external" href="https://github.com/TaylorOshan/pysal/tree/sparse_glm_spint/pysal/contrib/spint"&gt;branch&lt;/a&gt;. This required a custom version of the
categorical() function from statsmodels (exampe &lt;a class="reference external" href="https://gist.github.com/TaylorOshan/45cd01bb08e23280549aee770a05cdfe"&gt;here&lt;/a&gt;) to create the dummy variables needed for constrained spatial interaction models and then altering the least squares computations to make sure that no large dense arrays were being created throughout the routine. Specifically, it was necessary to change the order in which some of the operations were carried out to avoid the creations of large dense arrays. Now it is possible to calibrate constrained spatial interaction models using GLM's for larger N. The most demanding model estimation I have &lt;a class="reference external" href="https://gist.github.com/TaylorOshan/42d90dbf219b50f3b0d54e06ba4e8b5b"&gt;tested&lt;/a&gt; was a doubly-constrained model with 5000 locations, which implies N=25,000,000 and a design matrix with the dimensions of (25,000,000, 10001). Looking forward, I will further test this sparse GLM framework to see if there
are any losses assocaited with it when N is small to moderate.&lt;/p&gt;
&lt;p&gt;This week I also explored gradient optimization packages that coyld be used in
lieu of a GLM framework. So far the options seem to be between autograd/scipy or
Theano. I was able to create a working &lt;a class="reference external" href="https://gist.github.com/TaylorOshan/13c3b4a1d9489e03ea70ee52ecb0b61d"&gt;example&lt;/a&gt; in autograd/scipy, though this
has not been developed any further yet and will likely be pushed off until later
in the project. For now, the focus will remain on using GLM's for estimation.&lt;/p&gt;
&lt;p&gt;Next week I will begin to look at diagnostics for count models, zand
alternatives when we have overdispersed/zero-inflated dependent variables.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Taylor Oshan</dc:creator><pubDate>Fri, 03 Jun 2016 16:12:00 -0700</pubDate><guid>tag:,2016-06-03:sparse_glm.html</guid><category>GSOC</category></item><item><title>SpInt Framework</title><link>/framework.html</link><description>&lt;p&gt;Given the API deisgn discussed in the previous post, the initial few days of
coding were used to build the general framework and core classes that will be
used in SpInt. Since the gravity-type models make up the majority of existig
model specifications, the initial focus for developing the general framework
essentially means focusing on developing classes for gravity models.&lt;/p&gt;
&lt;p&gt;It was decided to split the four primary model specfications (unconstrained or
traditional gravity model, production-constrained, attraction-constrained,
doubly-constrained) into four separate &amp;quot;user&amp;quot; style classes (see pysal.spreg
module) that inherit from one base class (BaseGravity). These user classes
serve to accept structured array inputs and configurations from the user and
then carry out model-specific checks of the data. Then they pass the inputs into
an __init__ method for BaseGravity. Depending on which inputs are passed and the
type of user class that __init__ is called from, BaseGravity then further checks the
data and preapres it into an endogenous dependent variable, y, and and a
set of exogenous explanatory variables, X. Finally, BaseGravity then calls an
__init__ method to the class that it inherits from, CountModel, with these newly
formatted inputs and any estimation options, and a fit() method. The init_method
carries out a simple check to make sure the dependent variable, y, is indeed a
count-based variable and the fit method carries out the selected estimation
routine. Currently the default estimatation framework/method is MLE using the generalized linear model (GLM)/iteratively re-weighted least squares (IWLS) in statmodels,
though others can be added such as GLM/gradient search or non-linear
formulations/gradient search.&lt;/p&gt;
&lt;p&gt;The design was carried out in this manner (CountModel --&amp;gt; BaseGravity --&amp;gt;
Gravity/Production/Attraction/Doubly) in order to promote flexibility and modularity.
For example, CountModel can be expanded by adding tests for overdisperison,
additional estimation routines and count-based probability models other than
Poisson (i.e., negative binomial), which can be useful throughout pysal for other count-based
modeling tasks. This also means that SpInt is not a priori limited to any single
type of estimation technique or probability model in the future. Then the BaseGravity
class does the heavy lifting in terms of data munging and common data integrtiy
checks. And finally, the user classes aim to restrcit input for specific types
of gravity models, carry out model-specific checks, and then to organize and
prepare the model results for the user.&lt;/p&gt;
&lt;p&gt;This code and an exmple of its use in an ipython notebook can be found &lt;a class="reference external" href="https://github.com/TaylorOshan/pysal/blob/glm_spint/pysal/contrib/spint"&gt;here&lt;/a&gt;. At
the moment estimation can only be carried out using a GLM/IWLS in statmodels and
only basic results are available, though a more user-friendly presentation of
the results will be created, such as the results.summary() method in
statsmodels.&lt;/p&gt;
&lt;p&gt;Looking forward, this framework will be filled out with the additons already
described as well potentially adding a mechanism that allows users to flexibly
use different input formats. Currently, input consists of all arrays, but it
would be helpful to allow users to pass in a pandas DataFrames and the names of the columns that refer to different arrays. I will also begin
to explore sparse data structures/algebra and when they will be the most
beneficial to employ.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Taylor Oshan</dc:creator><pubDate>Fri, 27 May 2016 08:45:00 -0700</pubDate><guid>tag:,2016-05-27:framework.html</guid><category>GSOC</category></item><item><title>Initial GSOC meeting - API Design</title><link>/api-design.html</link><description>&lt;p&gt;During the community bonding period of the GSOC program I have been thinking
largely about the design of the API for SpInt.&lt;/p&gt;
&lt;p&gt;First, the API would need to accommodate a basic gravity model and then also support
extensions that include SAR terms, a spatial filter term, or a competing
destinations term. Typically the competing destinations term is nothing more
than a sum of distance weighted attributes, which should be simple. I think it
shouldn't be too hard use the existing pysal spatial weights module as a base to
create a function to create OD-based SAR spatial weights. And there is some code
available for computing spatial filters in &lt;a class="reference external" href="https://github.com/bchastain/esf"&gt;python&lt;/a&gt;. So I think in order to accommodate the three
extensions it might be simplest to have the user first compute these and then
pass them in  as optional arguments to the basic gravity model. In the case of
the competing destinations or spatial filter model, there is no new estimator
required and so its as easy as including the extra variable (within an OLS or
Poisson regression framework). If the optional SAR term is passed in, then under
the hood a new estimator could be used rather than the default OLS or  Poisson
estimator. I imagine the basic call could look something like&lt;/p&gt;
&lt;p&gt;spint.gravity(required_arguments, cd=None, sf=None, w=None)&lt;/p&gt;
&lt;p&gt;where cd would be an optional competing destinations variable, sf the optional
spatial filter term, and w would be optional spatial weight, each of which would
be pre-computed using code from the weights module or other utility functions.&lt;/p&gt;
&lt;p&gt;The other alternative could be to have separate calls for each extension where
the corresponding optional term now becomes required:&lt;/p&gt;
&lt;p&gt;spint.gravity(required_arguments)
spint.gravity.cd(required_arguments)
spint.gravity.sf(required_arguments)
spint.gravity.lag(required_arguments)&lt;/p&gt;
&lt;p&gt;I am inclined to think the former is simpler, esepcially if one would like to
combine models.&lt;/p&gt;
&lt;p&gt;Another layer to the API would be each &amp;quot;member&amp;quot; to the Wilson-type &amp;quot;family&amp;quot; of
models: unconstrained or basic gravity model, production or origin constrained,
attraction or destination constrained, and doubly constrained. This is achieved
technically by adding in either balancing factors or fixed effects during
estimation, depending on the estimation technique. Then building from the first
two examples the API could look like either:&lt;/p&gt;
&lt;ol class="arabic simple"&gt;
&lt;li&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;spint.gravity.unconstrained(required_arguments, cd=None, sf=None, w=None)&lt;/p&gt;
&lt;p&gt;spint.gravity.production(required_arguments, cd=None, sf=None, w=None)&lt;/p&gt;
&lt;p&gt;spint.gravity.attraction(required_arguments, cd=None, sf=None, w=None)&lt;/p&gt;
&lt;p&gt;spint.gravity.doubly(required_arguments, cd=None, sf=None, w=None)&lt;/p&gt;
&lt;ol class="arabic simple" start="2"&gt;
&lt;li&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;spint.gravity(required_arguments, constraint=none)&lt;/p&gt;
&lt;p&gt;spint.gravity.cd(required_arguments, constraint=none)&lt;/p&gt;
&lt;p&gt;spint.gravity.sf(required_arguments, constraint=none)&lt;/p&gt;
&lt;p&gt;spint.gravity.lag(required_arguments, constraint=none)&lt;/p&gt;
&lt;p&gt;I think that option (1) may be more intuitive because the
doubly-constrained model is unique in that it requires a squared OD matrix (all
origins are also destinations) and so it might be natural since each of the four
varieties may have different input properties that need to be checked.&lt;/p&gt;
&lt;p&gt;In terms of model fitting techniques, I think it would be neat to use something
similar to stats models like:&lt;/p&gt;
&lt;p&gt;model = spint.gravity.unconstrained(required_arguments, cd=None, sf=None,
w=None)&lt;/p&gt;
&lt;p&gt;results = model.fit(fit_arguments)&lt;/p&gt;
&lt;p&gt;which would be natural if everything is being built onto of a GLM framework. The
reasoning behind using a GLM framework was to be able take advantage of
additional count models such as negative binomial relatively easily.
&amp;quot;fit_arguments&amp;quot; could include the type of probability model (gaussian, poisson,
negative binomial, etc.), and the fit technique (iteratively re-weighted least
squares or Theano/Autograd MLE). It could then be possible to build a higher
level of abstraction on top of this that more closely matches the base/user
class organization used in the spreg module within pysal.&lt;/p&gt;
&lt;p&gt;Aside from gravity models, there are other spatial interaction models, but that I sort of think of as separate
from those described above because they are non-parametric methods, which are either
deterministic &amp;quot;universal&amp;quot; models that mostly come out of the human mobility
literature or use a neural network approach. I think it would
make sense to accommodate non-parametric  models so that they are separate from
gravity models. For example:&lt;/p&gt;
&lt;p&gt;spint.mobility.radiaton()&lt;/p&gt;
&lt;p&gt;spint.mobility.inv_pop_weighted()&lt;/p&gt;
&lt;p&gt;spint.neural()&lt;/p&gt;
&lt;p&gt;To summarize, the API needs to accomodate the &amp;quot;family&amp;quot; of spatial interaction
model varieties (unconstrained, production-constrained, attraction-constrained,
doubly-constrained), where each of these varieties can then have several
extensions to include a competing destinationas terms, a spatial filtering term,
or a SAR term. The API also needs to acommodate non-parametric methods such as
dterminstic models and neural network based models. Framework (1) from above
will be adopted first as the main API.&lt;/p&gt;
&lt;p&gt;During the first GSOC meeting with my mentors we decided it would be best to
first get as many model variations working as possible. Then we could tweak the
API accoridngly and focus on optimizing the computational speed. We've also set
up a Trello board so that we can collaborate on assigning/managing tasks for the
project.&lt;/p&gt;
&lt;p&gt;Finally, I have started a wiki &lt;a class="reference external" href="https://github.com/pysal/pysal/wiki/SpInt-Development"&gt;page&lt;/a&gt; on the pysal group on Github where it will
be possible to discuss model specifications and their related literature, such
as the Poisson SAR lag model.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Taylor Oshan</dc:creator><pubDate>Fri, 20 May 2016 11:23:00 -0700</pubDate><guid>tag:,2016-05-20:api-design.html</guid><category>GSOC</category></item><item><title>Hello World</title><link>/hello-world.html</link><description>&lt;p&gt;Hello, world! This blog will be used to document the implementation of a spatial
interaction (SpInt) modeling toolkit as part of the Python Spatial Analysis
Library (PySAL)  project, hopefully with assistance from Google through the Google
Summer of Code Program. Stay tuned!&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Taylor Oshan</dc:creator><pubDate>Mon, 21 Mar 2016 22:01:00 -0700</pubDate><guid>tag:,2016-03-21:hello-world.html</guid><category>GSOC</category></item></channel></rss>