<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>Building SpInt</title><link href="/" rel="alternate"></link><link href="/feeds/all.atom.xml" rel="self"></link><id>/</id><updated>2016-05-27T08:45:00-07:00</updated><entry><title>SpInt Framework</title><link href="/framework.html" rel="alternate"></link><updated>2016-05-27T08:45:00-07:00</updated><author><name>Taylor Oshan</name></author><id>tag:,2016-05-27:framework.html</id><summary type="html">&lt;p&gt;Given the API deisgn discussed in the previous post, the initial few days of
coding were used to build the general framework and core classes that will be
used in SpInt. Since the gravity-type models make up the majority of existig
model specifications, the initial focus for developing the general framework
essentially means focusing on developing classes for gravity models.&lt;/p&gt;
&lt;p&gt;It was decided to split the four primary model specfications (unconstrained or
traditional gravity model, production-constrained, attraction-constrained,
doubly-constrained) into four separate &amp;quot;user&amp;quot; style classes (see pysal.spreg
module) that inherit from one base class (BaseGravity). These user classes
serve to accept structured array inputs and configurations from the user and
then carry out model-specific checks of the data. Then they pass the inputs into
an __init__ method for BaseGravity. Depending on which inputs are passed and the
type of user class that __init__ is called from, BaseGravity then further checks the
data and preapres it into an endogenous dependent variable, y, and and a
set of exogenous explanatory variables, X. Finally, BaseGravity then calls an
__init__ method to the class that it inherits from, CountModel, with these newly
formatted inputs and any estimation options, and a fit() method. The init_method
carries out a simple check to make sure the dependent variable, y, is indeed a
count-based variable and the fit method carries out the selected estimation
routine. Currently the default estimatation framework/method is MLE using the generalized linear model (GLM)/iteratively re-weighted least squares (IWLS) in statmodels,
though others can be added such as GLM/gradient search or non-linear
formulations/gradient search.&lt;/p&gt;
&lt;p&gt;The design was carried out in this manner (CountModel --&amp;gt; BaseGravity --&amp;gt;
Gravity/Production/Attraction/Doubly) in order to promote flexibility and modularity.
For example, CountModel can be expanded by adding tests for overdisperison,
additional estimation routines and count-based probability models other than
Poisson (i.e., negative binomial), which can be useful throughout pysal for other count-based
modeling tasks. This also means that SpInt is not a priori limited to any single
type of estimation technique or probability model in the future. Then the BaseGravity
class does the heavy lifting in terms of data munging and common data integrtiy
checks. And finally, the user classes aim to restrcit input for specific types
of gravity models, carry out model-specific checks, and then to organize and
prepare the model results for the user.&lt;/p&gt;
&lt;p&gt;This code and an exmple of its use in an ipython notebook can be found &lt;a class="reference external" href="https://github.com/TaylorOshan/pysal/blob/glm_spint/pysal/contrib/spint"&gt;here&lt;/a&gt;. At
the moment estimation can only be carried out using a GLM/IWLS in statmodels and
only basic results are available, though a more user-friendly presentation of
the results will be created, such as the results.summary() method in
statsmodels.&lt;/p&gt;
&lt;p&gt;Looking forward, this framework will be filled out with the additons already
described as well potentially adding a mechanism that allows users to flexibly
use different input formats. Currently, input consists of all arrays, but it
would be helpful to allow users to pass in a pandas DataFrames and the names of the columns that refer to different arrays. I will also begin
to explore sparse data structures/algebra and when they will be the most
beneficial to employ.&lt;/p&gt;
</summary><category term="GSOC"></category></entry><entry><title>Initial GSOC meeting - API Design</title><link href="/api-design.html" rel="alternate"></link><updated>2016-05-20T11:23:00-07:00</updated><author><name>Taylor Oshan</name></author><id>tag:,2016-05-20:api-design.html</id><summary type="html">&lt;p&gt;During the community bonding period of the GSOC program I have been thinking
largely about the design of the API for SpInt.&lt;/p&gt;
&lt;p&gt;First, the API would need to accommodate a basic gravity model and then also support
extensions that include SAR terms, a spatial filter term, or a competing
destinations term. Typically the competing destinations term is nothing more
than a sum of distance weighted attributes, which should be simple. I think it
shouldn't be too hard use the existing pysal spatial weights module as a base to
create a function to create OD-based SAR spatial weights. And there is some code
available for computing spatial filters in &lt;a class="reference external" href="https://github.com/bchastain/esf"&gt;python&lt;/a&gt;. So I think in order to accommodate the three
extensions it might be simplest to have the user first compute these and then
pass them in  as optional arguments to the basic gravity model. In the case of
the competing destinations or spatial filter model, there is no new estimator
required and so its as easy as including the extra variable (within an OLS or
Poisson regression framework). If the optional SAR term is passed in, then under
the hood a new estimator could be used rather than the default OLS or  Poisson
estimator. I imagine the basic call could look something like&lt;/p&gt;
&lt;p&gt;spint.gravity(required_arguments, cd=None, sf=None, w=None)&lt;/p&gt;
&lt;p&gt;where cd would be an optional competing destinations variable, sf the optional
spatial filter term, and w would be optional spatial weight, each of which would
be pre-computed using code from the weights module or other utility functions.&lt;/p&gt;
&lt;p&gt;The other alternative could be to have separate calls for each extension where
the corresponding optional term now becomes required:&lt;/p&gt;
&lt;p&gt;spint.gravity(required_arguments)
spint.gravity.cd(required_arguments)
spint.gravity.sf(required_arguments)
spint.gravity.lag(required_arguments)&lt;/p&gt;
&lt;p&gt;I am inclined to think the former is simpler, esepcially if one would like to
combine models.&lt;/p&gt;
&lt;p&gt;Another layer to the API would be each &amp;quot;member&amp;quot; to the Wilson-type &amp;quot;family&amp;quot; of
models: unconstrained or basic gravity model, production or origin constrained,
attraction or destination constrained, and doubly constrained. This is achieved
technically by adding in either balancing factors or fixed effects during
estimation, depending on the estimation technique. Then building from the first
two examples the API could look like either:&lt;/p&gt;
&lt;ol class="arabic simple"&gt;
&lt;li&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;spint.gravity.unconstrained(required_arguments, cd=None, sf=None, w=None)&lt;/p&gt;
&lt;p&gt;spint.gravity.production(required_arguments, cd=None, sf=None, w=None)&lt;/p&gt;
&lt;p&gt;spint.gravity.attraction(required_arguments, cd=None, sf=None, w=None)&lt;/p&gt;
&lt;p&gt;spint.gravity.doubly(required_arguments, cd=None, sf=None, w=None)&lt;/p&gt;
&lt;ol class="arabic simple" start="2"&gt;
&lt;li&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;spint.gravity(required_arguments, constraint=none)&lt;/p&gt;
&lt;p&gt;spint.gravity.cd(required_arguments, constraint=none)&lt;/p&gt;
&lt;p&gt;spint.gravity.sf(required_arguments, constraint=none)&lt;/p&gt;
&lt;p&gt;spint.gravity.lag(required_arguments, constraint=none)&lt;/p&gt;
&lt;p&gt;I think that option (1) may be more intuitive because the
doubly-constrained model is unique in that it requires a squared OD matrix (all
origins are also destinations) and so it might be natural since each of the four
varieties may have different input properties that need to be checked.&lt;/p&gt;
&lt;p&gt;In terms of model fitting techniques, I think it would be neat to use something
similar to stats models like:&lt;/p&gt;
&lt;p&gt;model = spint.gravity.unconstrained(required_arguments, cd=None, sf=None,
w=None)&lt;/p&gt;
&lt;p&gt;results = model.fit(fit_arguments)&lt;/p&gt;
&lt;p&gt;which would be natural if everything is being built onto of a GLM framework. The
reasoning behind using a GLM framework was to be able take advantage of
additional count models such as negative binomial relatively easily.
&amp;quot;fit_arguments&amp;quot; could include the type of probability model (gaussian, poisson,
negative binomial, etc.), and the fit technique (iteratively re-weighted least
squares or Theano/Autograd MLE). It could then be possible to build a higher
level of abstraction on top of this that more closely matches the base/user
class organization used in the spreg module within pysal.&lt;/p&gt;
&lt;p&gt;Aside from gravity models, there are other spatial interaction models, but that I sort of think of as separate
from those described above because they are non-parametric methods, which are either
deterministic &amp;quot;universal&amp;quot; models that mostly come out of the human mobility
literature or use a neural network approach. I think it would
make sense to accommodate non-parametric  models so that they are separate from
gravity models. For example:&lt;/p&gt;
&lt;p&gt;spint.mobility.radiaton()&lt;/p&gt;
&lt;p&gt;spint.mobility.inv_pop_weighted()&lt;/p&gt;
&lt;p&gt;spint.neural()&lt;/p&gt;
&lt;p&gt;To summarize, the API needs to accomodate the &amp;quot;family&amp;quot; of spatial interaction
model varieties (unconstrained, production-constrained, attraction-constrained,
doubly-constrained), where each of these varieties can then have several
extensions to include a competing destinationas terms, a spatial filtering term,
or a SAR term. The API also needs to acommodate non-parametric methods such as
dterminstic models and neural network based models. Framework (1) from above
will be adopted first as the main API.&lt;/p&gt;
&lt;p&gt;During the first GSOC meeting with my mentors we decided it would be best to
first get as many model variations working as possible. Then we could tweak the
API accoridngly and focus on optimizing the computational speed. We've also set
up a Trello board so that we can collaborate on assigning/managing tasks for the
project.&lt;/p&gt;
&lt;p&gt;Finally, I have started a wiki &lt;a class="reference external" href="https://github.com/pysal/pysal/wiki/SpInt-Development"&gt;page&lt;/a&gt; on the pysal group on Github where it will
be possible to discuss model specifications and their related literature, such
as the Poisson SAR lag model.&lt;/p&gt;
</summary><category term="GSOC"></category></entry><entry><title>Hello World</title><link href="/hello-world.html" rel="alternate"></link><updated>2016-03-21T22:01:00-07:00</updated><author><name>Taylor Oshan</name></author><id>tag:,2016-03-21:hello-world.html</id><summary type="html">&lt;p&gt;Hello, world! This blog will be used to document the implementation of a spatial
interaction (SpInt) modeling toolkit as part of the Python Spatial Analysis
Library (PySAL)  project, hopefully with assistance from Google through the Google
Summer of Code Program. Stay tuned!&lt;/p&gt;
</summary><category term="GSOC"></category></entry></feed>